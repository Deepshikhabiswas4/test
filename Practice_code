from pyspark.sql.functions import col, substring, trim, upper
from pyspark.sql.types import StringType, DecimalType, DateType

# =====================================================
# Spark & common utilities (already defined in common)
# =====================================================
spark = create_spark()
HDFS_BASE = get_hdfs_base()

ORACLE_URL = oracle_url
ORACLE_USER = oracle_user
ORACLE_PASSWORD = oracle_password
ORACLE_DRIVER = oracle_driver

# =====================================================
# Paths
# =====================================================
GLIF_DELTA_PATH = f"{HDFS_BASE}/glif/delta"

# =====================================================
# Read GLIF using EXISTING common function
# (DO NOT change this part in your pipeline)
# =====================================================
raw_df = df_raw   # coming from your 600-line code

# Ensure consistent column name
raw_df = raw_df.withColumnRenamed(raw_df.columns[0], "raw_record")

# =====================================================
# Read substring metadata from DB
# =====================================================
metadata_query = """
(
    SELECT
        UPPER(COLUMNNAME)   AS COLUMNNAME,
        STARTVALUE,
        ENDVALUE,
        UPPER(DATA_TYPE)    AS DATA_TYPE
    FROM FINCORE.TOTALDATAPARAMETERS
    WHERE UPPER(TOBEINCLUDED) = 'Y'
    ORDER BY STARTVALUE
) T
"""

metadata_df = spark.read.format("jdbc") \
    .option("url", ORACLE_URL) \
    .option("dbtable", metadata_query) \
    .option("user", ORACLE_USER) \
    .option("password", ORACLE_PASSWORD) \
    .option("driver", ORACLE_DRIVER) \
    .load()

metadata = metadata_df.collect()

if not metadata:
    raise Exception("No GLIF metadata found in DB. Aborting parse.")

# =====================================================
# Build dynamic substring expressions
# =====================================================
select_exprs = []

for row in metadata:
    col_name = row["COLUMNNAME"]
    start_pos = int(row["STARTVALUE"])
    end_pos = int(row["ENDVALUE"])
    data_type = row["DATA_TYPE"]

    length = end_pos - start_pos + 1

    expr_col = trim(
        substring(col("raw_record"), start_pos, length)
    )

    if data_type == "NUMBER":
        expr_col = expr_col.cast(DecimalType(25, 4))

    elif data_type == "DATE":
        expr_col = expr_col.cast(DateType())

    else:
        expr_col = expr_col.cast(StringType())

    select_exprs.append(expr_col.alias(col_name))

# =====================================================
# Create final parsed dataframe
# =====================================================
final_df = raw_df.select(*select_exprs)

# =====================================================
# Write to Delta (schema controlled by DB)
# =====================================================
final_df.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save(GLIF_DELTA_PATH)
