# import sys
# from pyspark.sql import SparkSession
# from pyspark.sql.functions import col
# from datetime import date, timedelta

# def main():
#     # Define the path where the Delta Lake table is saved
#     HDFS_PATH = "hdfs://10.177.103.199:8022/data-lake/Fincore/CBS_balance"

#     try:
#         # Initialize Spark Session with Delta Lake support
#         spark = SparkSession.builder \
#             .appName("Read CBS Balance Delta Lake Table") \
#             .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
#             .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
#             .getOrCreate()
        
#         print(f"Attempting to read Delta Lake table from: {HDFS_PATH}")
#         yesterday = date.today() - timedelta(days=1)
#         # Read the Delta Lake data into a DataFrame
#         df_delta_read = spark.read.format("delta").load(HDFS_PATH).filter(col("BALANCE_DATE") == '2025-12-01')

#         # Count the rows in the Delta Lake table
#         row_count = df_delta_read.count()
#         print(f"Successfully read Delta Lake table.")
#         print(f"Total rows in the Delta Lake table: {row_count}")

#         # Display the schema and a sample of the data
#         # print("\nDataFrame Schema:")
#         # df_delta_read.printSchema()

#         print("\nData Sample (5 rows):")
#         df_delta_read.show(truncate=False)
      

#         # Optional: You can filter the data based on the BALANCE_DATE column you added
#         # Example: Show only records stamped as 'yesterday'
#         # from datetime import date, timedelta
#         # yesterday = date.today() - timedelta(days=1)
#         # df_yesterday = df_delta_read.filter(col("BALANCE_DATE") == yesterday)
#         # print(f"\nRows for yesterday ({yesterday}): {df_yesterday.count()}")
#         # df_yesterday.show()

#     except Exception as e:
#         print(f"An error occurred while reading the Delta Lake table: {e}")
#         sys.exit(1)
#     finally:
#         if 'spark' in locals() and spark:
#             spark.stop()

# if __name__ == "__main__":
#     main()





import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, lit, sum as _sum

def main():
    HDFS_PATH1 = "hdfs://10.177.103.199:8022/data-lake/Fincore/CBS_balance"
    HDFS_PATH2 = "hdfs://10.177.103.199:8022/data-lake/Fincore/Cbs_balance"

    try:
        spark = SparkSession.builder \
            .appName("CBS Balance Validation") \
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
            .getOrCreate()

        # Define business dates
        today = "2025-12-02"
        yesterday = "2025-12-01"

        df = spark.read.format("delta").load(HDFS_PATH)

        df_yesterday = df.filter(to_date(col("BALANCE_DATE")) == to_date(lit(yesterday))) \
                         .select("GLCC", col("closing_balance").alias("yest_balance"))

        df_today = df.filter(to_date(col("BALANCE_DATE")) == to_date(lit(today))) \
                     .select("GLCC", col("closing_balance").alias("today_balance"))

        # Join both days
        comparison_df = df_yesterday.join(df_today, on="GLCC", how="inner") \
                         .withColumn("difference", col("today_balance") - col("yest_balance"))

        print("=============== Balance Comparison Results ===============")
        comparison_df.show(50, False)

        # Show mismatched balances (difference NOT zero)
        print("=============== Mismatch Records ===============")
        mismatch_df = comparison_df.filter(col("difference") != 0)
        mismatch_df.show(50, False)

        print("=============== Summary Counts ===============")
        print("Total GLCC entries:", comparison_df.count())
        print("Mismatch entries:", mismatch_df.count())

    except Exception as e:
        print(f"Error validating balances: {e}")
        sys.exit(1)
    finally:
        if 'spark' in locals():
            spark.stop()

if __name__ == "__main__":
    main()
