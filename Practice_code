import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from datetime import date, timedelta

def main():
    # Define the path where the Delta Lake table is saved
    HDFS_PATH = "hdfs://10.177.103.199:8022/data-lake/Fincore/Gl_Balance"

    try:
        # Initialize Spark Session with Delta Lake support
        spark = SparkSession.builder \
            .appName("Read GL Balance Delta Lake Table") \
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
            .getOrCreate()
        
        print(f"Attempting to read Delta Lake table from: {HDFS_PATH}")

        # Read the Delta Lake data into a DataFrame
        df_delta_read = spark.read.format("delta").load(HDFS_PATH)

        # Count the rows in the Delta Lake table
        row_count = df_delta_read.count()
        print(f"Successfully read Delta Lake table.")
        print(f"Total rows in the Delta Lake table: {row_count}")

        # Display the schema and a sample of the data
        # print("\nDataFrame Schema:")
        # df_delta_read.printSchema()

        print("\nData Sample (5 rows):")
        df_delta_read.show(20, False)

        # Optional: You can filter the data based on the BALANCE_DATE column you added
        # Example: Show only records stamped as 'yesterday'
        # from datetime import date, timedelta
        # yesterday = date.today() - timedelta(days=1)
        # df_yesterday = df_delta_read.filter(col("BALANCE_DATE") == yesterday)
        # print(f"\nRows for yesterday ({yesterday}): {df_yesterday.count()}")
        # df_yesterday.show()

    except Exception as e:
        print(f"An error occurred while reading the Delta Lake table: {e}")
        sys.exit(1)
    finally:
        if 'spark' in locals() and spark:
            spark.stop()

if __name__ == "__main__":
    main()
