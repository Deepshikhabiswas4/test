import concurrent.futures
import builtins
import re

from datetime import datetime, date, timedelta

from pyspark.sql.functions import col, lit, concat, broadcast
from pyspark.sql.types import DateType, DecimalType

# ==========================================================
# Common Methods (Same Style as Your Pipeline)
# ==========================================================

from common.logger import setup_logger
from common.properties import get_oracle_properties
from common.dateUtil import get_etl_date
from common.processRun import get_run_id
from common.createSpark import create_spark_session
from common.fileUtil import get_hdfs_base
from common.logdb import log_etl

# Oracle Common Read/Write
from common.read_write_oracle import read_oracle
from common.read_write_oracle import write_oracle

# ==========================================================
# Spark Session + ETL Date Setup
# ==========================================================

HDFS_BASE = get_hdfs_base()

spark = create_spark_session("GLIF_PIPELINE", HDFS_BASE)

today = get_etl_date(spark)
etl_plus_1_date = today + timedelta(days=1)

posting_date_str = today.strftime("%Y-%m-%d")
etl_plus_1_str = etl_plus_1_date.strftime("%Y-%m-%d")

RUN_ID = get_run_id(spark, posting_date_str, "GLIF")

logger = setup_logger("GLIF", RUN_ID, posting_date_str)

startTime = datetime.now()

logger.info(f"=== Using current date for POST_DATE: {posting_date_str} ===")
logger.info(f"=== RUN_ID as : {RUN_ID} ===")

log_etl(
    spark,
    "GLIF",
    RUN_ID,
    "OPENING BALANCE FORWARD JOB STARTED",
    5,
    1,
    startTime
)

# ==========================================================
# Oracle Properties Setup
# ==========================================================

logger.info("Fetched oracle properties SUCCESSFULLY :)")

oracle_properties = get_oracle_properties()
logger.info(oracle_properties)

# ==========================================================
# Step 1: Read Closing Balance from Oracle GL_BALANCE
# ==========================================================

logger.info("============================================")
logger.info("Step 1: Reading Closing Balance from GL_BALANCE")
logger.info("============================================")

gl_balance_query = f"""
(
    SELECT
        CGL,
        CURRENCY,
        BRANCH_CODE,
        BALANCE,
        INR_BALANCE
    FROM GL_BALANCE
    WHERE BALANCE_DATE = TO_DATE('{posting_date_str}', 'YYYY-MM-DD')
) T1
"""

closing_balance_df = read_oracle(spark, gl_balance_query)

logger.info("Closing Balance Read Successfully")
logger.info(f"Closing Balance Count = {closing_balance_df.count()}")

# ==========================================================
# Step 2: Create Opening Balance for ETL+1
# ==========================================================

logger.info("============================================")
logger.info("Step 2: Creating Opening Balance for ETL+1")
logger.info("============================================")

opening_balance_df = closing_balance_df.withColumn(
    "BALANCE_DATE",
    lit(etl_plus_1_str).cast(DateType())
)

logger.info("Opening Balance DF Created Successfully")

# ==========================================================
# Step 3: Year-End Exception Logic (31st March)
# ==========================================================

if today.month == 3 and today.day == 31:

    logger.info("============================================")
    logger.info("Step 3: Year-End Detected (31st March)")
    logger.info("Applying BAL_FWD Rule")
    logger.info("============================================")

    cal_query = """
    (SELECT ACTIVE_FLAG FROM CALENDER_CONFIG WHERE ACTIVE_FLAG = 1) T1
    """

    cal_active_df = read_oracle(spark, cal_query)

    if cal_active_df.count() > 0:

        logger.info("Active Financial Year Found")

        cgl_master_query = """
        (SELECT CGL_NUMBER, BAL_FWD FROM CGL_MASTER) T1
        """

        cgl_master_df = read_oracle(spark, cgl_master_query)

        opening_balance_df = opening_balance_df.join(
            broadcast(cgl_master_df),
            opening_balance_df["CGL"] == cgl_master_df["CGL_NUMBER"],
            "inner"
        ).filter(
            col("BAL_FWD") == 1
        ).select(opening_balance_df["*"])

        logger.info("BAL_FWD Filter Applied Successfully")

    else:
        logger.info("No Active Financial Year Found â†’ Skipping BAL_FWD Filter")

# ==========================================================
# Step 4: Write Opening Balance into Delta (Option B)
# ==========================================================

logger.info("============================================")
logger.info("Step 4: Writing Opening Balance into Delta Lake")
logger.info("Overwrite Partition Option B Enabled")
logger.info("============================================")

GL_DELTA_PATH = f"{HDFS_BASE}/glif/opening_balance/"

delta_final_df = opening_balance_df.select(
    concat(col("BRANCH_CODE"), col("CURRENCY"), col("CGL")).alias("GLCC"),
    col("BALANCE").cast(DecimalType(25, 4)).alias("CLOSING_BALANCE"),
    col("BALANCE_DATE")
)

spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

delta_final_df.write.format("delta") \
    .mode("overwrite") \
    .option("replaceWhere", f"BALANCE_DATE = '{etl_plus_1_str}'") \
    .save(GL_DELTA_PATH)

logger.info(f"Delta Overwrite Done Successfully for {etl_plus_1_str}")

# ==========================================================
# Step 5: Write Opening Balance into Oracle BALANCE Table (Append Only)
# ==========================================================

logger.info("============================================")
logger.info("Step 5: Writing Opening Balance into Oracle BALANCE Table")
logger.info("Append Mode Enabled (No Overwrite)")
logger.info("============================================")

oracle_target_df = opening_balance_df.select(
    col("CGL"),
    col("CURRENCY"),
    col("BRANCH_CODE"),
    col("BALANCE_DATE"),
    col("BALANCE"),
    col("INR_BALANCE")
)

db_status = write_oracle(oracle_target_df, "BALANCE")

logger.info(db_status)

logger.info("Oracle Insert Completed Successfully")

# ==========================================================
# Job Completed
# ==========================================================

logger.info("============================================")
logger.info("OPENING BALANCE FORWARD JOB COMPLETED")
logger.info("============================================")

log_etl(
    spark,
    "GLIF",
    RUN_ID,
    "OPENING BALANCE FORWARD JOB COMPLETED",
    5,
    2,
    datetime.now()
)

spark.stop()
