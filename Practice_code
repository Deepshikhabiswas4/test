import os
from datetime import datetime
from pyspark.sql import DataFrame  # just for typing if needed

try:
    # 1. Create run date folder
    run_date = datetime.today().strftime("%Y-%m-%d")
    base_path = "hdfs://10.177.103.199:8022/data-lake/Fincore/spark_cluster/csvcheck1"
    output_path = f"{base_path}/run_date={run_date}"

    # Make sure the folder exists (for local FS, use os.makedirs; for HDFS you may need hdfs commands or Hadoop API)
    # Here is local folder creation for reference:
    # os.makedirs(output_path, exist_ok=True)
    # For HDFS, create folder using PySpark Hadoop API
    hadoop_conf = spark._jsc.hadoopConfiguration()
    fs = org.apache.hadoop.fs.FileSystem.get(hadoop_conf)
    path = org.apache.hadoop.fs.Path(output_path)
    if not fs.exists(path):
        fs.mkdirs(path)

    # 2. Trigger Kafka after folder creation
    producer = Producer({"bootstrap.servers": "kafkalistener"})
    producer.produce("pipeline-events", "Difference")
    producer.flush()

except Exception as e:
    logger.error(f"Error while writing in DB or Kafka stream: {e}")
    # Save DataFrame as a single CSV inside the run date folder
    final_df.coalesce(1) \
        .write \
        .format("csv") \
        .option("delimiter", "|") \
        .option("header", "true") \
        .mode("append") \
        .save(output_path)
