run_date = today.strftime("%Y-%m-%d")

base_path = "hdfs://10.177.103.199:8022/data-lake/fincore/spark_cluster/csvcheck1"
output_path = f"{base_path}/run_date={run_date}/{run_id}"

hadoop_conf = spark._jsc.hadoopConfiguration()
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)
Path = spark._jvm.org.apache.hadoop.fs.Path

output_dir = Path(output_path)
if not fs.exists(output_dir):
    fs.mkdirs(output_dir)

# ---------------- KAFKA IDPOTENCY MARKER ----------------
kafka_marker = Path(output_path + "/_KAFKA_TRIGGERED")

if fs.exists(kafka_marker):
    logger.info("Kafka already triggered for this run_id. Skipping Kafka trigger.")
else:
    try:
        producer = Producer({"bootstrap.servers": kafkalistener})
        producer.produce("pipeline-events", "Difference")
        producer.flush()
        logger.info("Kafka trigger sent successfully")

        # Create marker file so Kafka doesn't trigger again on rerun
        out_stream = fs.create(kafka_marker)
        out_stream.write(bytearray("triggered", "utf-8"))
        out_stream.close()

        logger.info("Kafka marker file created successfully")

    except Exception as e:
        logger.error(f"Kafka trigger failed: {e}")
        raise e   # stop job if kafka trigger itself fails

# ---------------- WRITE CSV (RETRY SAFE) ----------------
final_df.coalesce(1) \
    .write \
    .format("csv") \
    .option("delimiter", "|") \
    .option("header", "true") \
    .mode("overwrite") \
    .save(output_path)

# ---------------- RENAME PART FILE ----------------
for file_status in fs.listStatus(output_dir):
    filename = file_status.getPath().getName()

    if filename.startswith("part-") and filename.endswith(".csv"):
        src_path = file_status.getPath()
        dest_path = Path(output_path + "/difference.csv")

        if fs.exists(dest_path):
            fs.delete(dest_path, True)

        fs.rename(src_path, dest_path)
        logger.info(f"Renamed {filename} -> difference.csv")
        break
