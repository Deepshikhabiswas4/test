#!/usr/bin/env python3

import sys
import subprocess
import concurrent.futures
from datetime import date, timedelta, datetime
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.functions import expr, col, when, trim, substring, lit, sum, to_date
from pyspark.sql.types import DecimalType, StringType, StructType, StructField, LongType, FloatType
from pyspark.storagelevel import StorageLevel

# ============================ CONFIG & DATE ============================
today = date.today()
posting_date = today - timedelta(days=1)
posting_date_str = posting_date.strftime("%Y-%m-%d")

HDFS_BASE = "hdfs:///10.177.120.139:8022"
CBS_BALANCE_DATALAKE_PATH = f"{HDFS_BASE}/data-lake/4thercrunlake/CBS_balance"
SOURCE_PATH = f"{HDFS_BASE}/CBS-FILES/2025-11-28/BANCS24/"

oracle_url = "jdbc:oracle:thin:@10.177.120.192:1523/fincomprodba"
oracle_user = "fincoce"
oracle_password = "Password123"
oracle_driver = "oracle.jdbc.driver.OracleDriver"

# ============================ SPARK SESSION ============================
def create_spark_session(app_name, hdfs_uri):
    return (
        SparkSession.builder.appName(app_name)
        .config("spark.hadoop.fs.defaultFS", hdfs_uri)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .getOrCreate()
    )

# ============================ FUNCTIONS ============================
def invtogl(spark):
    try:
        df_raw = spark.read.text(f"{SOURCE_PATH}/BANCS24/INV")

        id_col = expr("substring(value, 200, 18)")
        amount_str = expr("substring(value, 3, 17)")
        value_type = expr("substring(value, 19, 1)")

        df_final = df_raw.select(
            id_col.alias("Id"),
            when(value_type == "-", -1 * trim(amount_str).cast(DecimalType(22, 4)) / 1000)
            .otherwise(trim(amount_str).cast(DecimalType(22, 4)) / 1000)
            .alias("Amount"),
        )
        return df_final
    except Exception as e:
        print(f"Error processing invtogl: {e}")
        raise

def bortogl(spark):
    try:
        df_raw = spark.read.text(f"{SOURCE_PATH}/BANCS24/BOR")

        id_col = expr("substring(value, 105, 18)")
        amount_str = expr("substring(value, 27, 17)")
        value_type = expr("substring(value, 44, 1)")

        df_final = df_raw.select(
            id_col.alias("Id"),
            when(value_type == "-", -1 * trim(amount_str).cast(DecimalType(22, 4)) / 10000)
            .otherwise(trim(amount_str).cast(DecimalType(22, 4)) / 10000)
            .alias("Amount"),
        )
        return df_final
    except Exception as e:
        print(f"Error processing bortogl: {e}")
        raise

def glccgen(spark):
    try:
        df_raw = spark.read.text(f"{SOURCE_PATH}/BANCS24/GLCGEN")

        id_col = expr("substring(value, 1, 18)")
        amount_str = expr("substring(value, length(value)-24, 17)")
        value_type = expr("substring(value, length(value), 1)")

        df_final = df_raw.select(
            id_col.alias("Id"),
            when(value_type == "-", -1 * trim(amount_str).cast(DecimalType(22, 4)) / 10000)
            .otherwise(trim(amount_str).cast(DecimalType(22, 4)) / 10000)
            .alias("Amount"),
        )
        return df_final
    except Exception as e:
        print(f"Error processing glccgen: {e}")
        raise

# ============================ MAIN ============================
if __name__ == "__main__":

    spark = create_spark_session("HDFS_Text_to_DataLake", HDFS_BASE)

    # ---------- PARALLEL FILE PROCESSING ----------
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            f1 = executor.submit(invtogl, spark)
            f2 = executor.submit(bortogl, spark)
            f3 = executor.submit(glccgen, spark)

            df_invtogl = f1.result()
            df_bortogl = f2.result()
            df_glccgen = f3.result()

        final_result_df = df_invtogl.unionByName(df_bortogl).unionByName(df_glccgen)

    except Exception as e:
        print(f"Error during parallel load: {e}")
        sys.exit(1)

    # ---------- COMPUTE TODAY'S BALANCE ----------
    try:
        final_aggregated_df = (
            final_result_df.select(
                col("Id").alias("GLCC"),
                col("Amount").cast(DecimalType(22, 3)).alias("closing_balance"),
            )
            .withColumn("BALANCE_DATE", to_date(lit(posting_date_str)))
        )

        # ------------------ WRITE TO DELTA ------------------
        final_aggregated_df.write \
            .format("delta") \
            .mode("append") \
            .save(CBS_BALANCE_DATALAKE_PATH)

        print(f"Successfully wrote to Delta Lake: {CBS_BALANCE_DATALAKE_PATH}")

        # ---------- CREATE AND UPLOAD MANIFEST FILE ----------
        print("\nCreating manifest file for Spark Stream trigger...")

        today_str = datetime.now().strftime("%Y%m%d")
        manifest_name = f"manifest_{today_str}_cbsbalance.txt"
        local_manifest = f"/tmp/{manifest_name}"

        with open(local_manifest, "w") as f:
            f.write("cbs successful")

        spark_stream_path = "hdfs://10.177.120.139:8022/data-lake/spark-stream"

        subprocess.run([
            "hdfs", "dfs", "-put", "-f",
            local_manifest,
            f"{spark_stream_path}/{manifest_name}"
        ], check=True)

        print(f"Manifest placed at: {spark_stream_path}/{manifest_name}")

    except Exception as e:
        print(f"Error during Delta or manifest stage: {e}")
        sys.exit(1)

    # ---------- INSERT INTO ORACLE ----------
    try:
        df_mapped = (
            final_aggregated_df
            .withColumn("BRANCH_CODE", substring(col("GLCC"), 1, 5))
            .withColumn("CURRENCY", substring(col("GLCC"), 6, 3))
            .withColumn("CGL", substring(col("GLCC"), 9, 10))
        )

        processed_df = df_mapped.select(
            "CGL", "BALANCE_DATE",
            col("closing_balance").alias("BALANCE"),
            "CURRENCY", "BRANCH_CODE"
        )

        processed_df.write \
            .format("jdbc") \
            .option("url", oracle_url) \
            .option("dbtable", "CBS_BALANCE") \
            .option("user", oracle_user) \
            .option("password", oracle_password) \
            .option("driver", oracle_driver) \
            .mode("append") \
            .save()

        print("Successfully wrote to Oracle DB")

    except Exception as e:
        print(f"Oracle write error: {e}")

    finally:
        spark.stop()

# Manifest file creation
from datetime import datetime
manifest_filename = "manufest_12dec2025_cbsbalance.txt"
manifest_content = "cbs successful"

# Write manifest to HDFS
hdfs_manifest_path = "<PUT_YOUR_HDFS_PATH_HERE>/" + manifest_filename
with open(manifest_filename, "w") as f:
    f.write(manifest_content)

# If using hdfs dfs command to put file
import subprocess
subprocess.run(["hdfs", "dfs", "-put", "-f", manifest_filename, hdfs_manifest_path])

