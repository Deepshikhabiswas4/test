import sys
from pyspark.sql import SparkSession
from datetime import date

# --- Configuration & Date Setup ---
today = date.today()
posting_date_str = today.strftime("%Y-%m-%d")
print("BALANCE DATE", posting_date_str)

HDFS_BASE = "hdfs://10.177.103.199:8022"
CBS_BALANCE_DATALAKE_PATH = f"{HDFS_BASE}/data-lake/Fincore/totalData/CBS_Data"
CBS_INV_DATALAKE_PATH     = f"{HDFS_BASE}/data-lake/Fincore/totalData/INV_Data"
CBS_BOTGL_DATALAKE_PATH   = f"{HDFS_BASE}/data-lake/Fincore/totalData/BOTGL_Data"

# ---------------- Spark Session Creation -------------------
spark = (
    SparkSession.builder.appName("TruncateDeltaPaths")
    .config("spark.hadoop.fs.defaultFS", HDFS_BASE)
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
)


def truncate_delta_path(path):
    """Removes all rows but keeps schema and structure"""
    print(f"Truncating data at path: {path}")
    
    # Read schema from delta
    df = spark.read.format("delta").load(path)
    
    # Create empty dataframe with same schema
    empty_df = df.limit(0)
    
    # Overwrite the data but retain the structure
    empty_df.write.format("delta").mode("overwrite").save(path)

    print(f"Successfully removed all data from: {path}\n")


# ----------- TRUNCATE ALL 3 DATALAKE PATHS ---------------
truncate_delta_path(CBS_BALANCE_DATALAKE_PATH)
truncate_delta_path(CBS_INV_DATALAKE_PATH)
truncate_delta_path(CBS_BOTGL_DATALAKE_PATH)

print("ALL DELTA DATA REMOVED SUCCESSFULLY â€” STRUCTURE RETAINED FOR ALL 3 DATASETS.")
