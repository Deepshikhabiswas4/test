from pyspark.sql import SparkSession

HDFS_BASE = "hdfs://10.177.103.199:8022"

GLCC_PATH     = f"{HDFS_BASE}/data-lake/Fincore/totalData/CBS_Data/GLCC"
INVTOGL_PATH  = f"{HDFS_BASE}/data-lake/Fincore/totalData/CBS_Data/INVTOGL"
BORTOGL_PATH  = f"{HDFS_BASE}/data-lake/Fincore/totalData/CBS_Data/BORTOGL"

# -------- Spark Session --------
spark = (
    SparkSession.builder.appName("Truncate-CBS-Delta")
    .config("spark.hadoop.fs.defaultFS", HDFS_BASE)
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
)

def clear_data(path: str):
    print(f"Cleaning Delta Data at: {path}")
    df = spark.read.format("delta").load(path)      # Read schema
    empty_df = df.limit(0)                          # Empty but same schema
    empty_df.write.format("delta").mode("overwrite").save(path)
    print(f"âœ“ Data removed from {path}, structure retained\n")

# -------- DELETE DATA FOR ALL 3 DATALAKES --------
clear_data(GLCC_PATH)
clear_data(INVTOGL_PATH)
clear_data(BORTOGL_PATH)

print("\n==== ALL DATA REMOVED. SCHEMA & DIRECTORIES RETAINED SUCCESSFULLY ====\n")
