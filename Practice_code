#!/usr/bin/env python3

from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.types import DecimalType
import traceback
import sys
from datetime import date, timedelta

def print_header(msg):
    print("\n" + "="*6 + " " + msg + " " + "="*6 + "\n")

def read_table_parallel_by_rownum(
    spark, jdbc_url, user, password, driver,
    difference, select_cols_sql, recon_date,
    num_partitions, fetchsize
):

    subquery = (
        "(\n"
        "  SELECT\n"
        "    {cols},\n"
        "    ROW_NUMBER() OVER (ORDER BY (BRANCH_CODE || '|' || CURRENCY || '|' || CGL)) rn\n"
        "  FROM {table}\n"
        "  WHERE RECON_RUN_DATE = TO_DATE('{date}','DD-MM-YY')\n"
        ") tmp"
    ).format(cols=select_cols_sql,
             table=difference,
             date=recon_date)

    print("################# PREDICATE PUSHDOWN QUERY #######################")
    print(subquery)
    print("##################################################################")

    ESTIMATED_UPPER_BOUND = 1000000

    df = (
        spark.read.format("jdbc")
            .option("url", jdbc_url)
            .option("dbtable", subquery)
            .option("user", user)
            .option("password", password)
            .option("driver", driver)
            .option("partitionColumn", "rn")
            .option("lowerBound", 1)
            .option("upperBound", ESTIMATED_UPPER_BOUND)
            .option("numPartitions", str(num_partitions))
            .option("fetchsize", str(fetchsize))
            .load()
    )
    return df

def main():
    try:
        spark = (
            SparkSession.builder
                .appName("Optimized Difference Amount Extract to Delta Lake")
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .getOrCreate()
        )

        spark.conf.set("spark.sql.adaptive.enabled", "true")
        spark.conf.set("spark.sql.shuffle.partitions", "200")

        # --- Configuration ---
        today = date.today()
        yesterday = today - timedelta(days=1)

        ORACLE_QUERY_DATE = "20-11-25"
        TARGET_DATE_VALUE = yesterday
        HDFS_PATH = "hdfs://10.177.103.199:8022/data-lake/Fincore/DiffrenceTable"

        ORACLE_USER = "fincore"
        ORACLE_PASSWORD = "Password#1234"
        ORACLE_DRIVER = "oracle.jdbc.driver.OracleDriver"
        JDBC_URL = "jdbc:oracle:thin:@//10.177.103.192:1523/fincorepdb1"

        NUM_PARTITIONS_READ = 5
        FETCHSIZE = 20000

        # Correct select columns
        select_cols_gl = (
            "BRANCH_CODE || CURRENCY || CGL AS GLCC, "
            "DIFFERENCE_AMOUNT, "
            "BRANCH_CODE, CURRENCY, CGL"
        )

        print_header("START DIFFERENCE AMOUNT DATA EXTRACTION")

        df_gl = read_table_parallel_by_rownum(
            spark, JDBC_URL, ORACLE_USER, ORACLE_PASSWORD, ORACLE_DRIVER,
            "FINCORE.DIFFERENCE", select_cols_gl,
            ORACLE_QUERY_DATE, NUM_PARTITIONS_READ, FETCHSIZE
        )

        print(f"Total records fetched: {df_gl.count()}")

        df_final = df_gl.select(
            F.lit(TARGET_DATE_VALUE).alias("RECON_DATE").cast("date"),
            F.col("BRANCH_CODE"),
            F.col("CURRENCY"),
            F.col("CGL"),
            F.col("GLCC"),
            F.col("DIFFERENCE_AMOUNT").cast(DecimalType(22, 3))
        )

        print_header("WRITE TO DELTA LAKE (HDFS)")

        df_final.write \
            .format("delta") \
            .mode("append") \
            .save(HDFS_PATH)

        print(f"Successfully appended data to Delta Lake at: {HDFS_PATH}")
        print_header("JOB COMPLETE")

        spark.stop()

    except Exception:
        print("ERROR during execution:")
        traceback.print_exc()
        try:
            spark.stop()
        except:
            pass
        sys.exit(1)

if __name__ == "__main__":
    print("START MAIN")
    main()
