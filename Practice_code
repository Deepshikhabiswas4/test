from pyspark.sql import SparkSession
from py4j.java_gateway import java_import

# Register Hadoop classes
java_import(spark._jvm, "org.apache.hadoop.fs.FileSystem")
java_import(spark._jvm, "org.apache.hadoop.fs.Path")

# Create manifest locally
manifest_filename = f"manifest_{today_str}_cbsbalance.txt"
with open(manifest_filename, "w") as f:
    f.write("cbs successful")

# HDFS destination
hdfs_manifest_path = f"hdfs://10.177.103.199:8022/data-lake/Fincore/StreamLake/CBS_balance/{manifest_filename}"

# Upload to HDFS using Hadoop FS API
fs = spark._jvm.FileSystem.get(spark._jsc.hadoopConfiguration())
fs.copyFromLocalFile(False, True, spark._jvm.Path(manifest_filename), spark._jvm.Path(hdfs_manifest_path))

print(f"Manifest file uploaded to: {hdfs_manifest_path}")
