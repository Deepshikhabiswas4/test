from pyspark.sql import functions as F
from pyspark.sql.types import (
    StringType,
    IntegerType,
    LongType,
    DecimalType
)

from common.properties import get_hdfs_base


def process_single_file_dynamic(
    spark,
    file_path,
    file_name
):
    """
    Reads ONE CBS / GLIF file, dynamically parses it using DB metadata,
    and returns:
      1) final dataframe (business columns only)
      2) target delta lake path
      3) record count
    """

    # -----------------------------------------------------------
    # 1. Detect FILE TYPE from file name
    # -----------------------------------------------------------
    file_name_upper = file_name.upper()

    if file_name_upper.startswith("GLIF"):
        file_type = "GLIF"
        delta_sub_path = "DATA-LAKE/GLIF"

    elif file_name_upper.startswith("INV"):
        file_type = "INV"
        delta_sub_path = "DATA-LAKE/BANCS24/INV"

    elif file_name_upper.startswith("BOR"):
        file_type = "BOR"
        delta_sub_path = "DATA-LAKE/BANCS24/BOR"

    elif file_name_upper.startswith("GLCCTA"):
        file_type = "GLCCTA"
        delta_sub_path = "DATA-LAKE/BANCS24/GLCCTA"

    elif file_name_upper.startswith("GLCCGEN"):
        file_type = "GLCCGEN"
        delta_sub_path = "DATA-LAKE/BANCS24/GLCCGEN"

    else:
        raise Exception(f"Unsupported file name: {file_name}")

    # -----------------------------------------------------------
    # 2. Resolve delta lake path
    # -----------------------------------------------------------
    HDFS_BASE = get_hdfs_base()
    target_delta_path = f"{HDFS_BASE}/{delta_sub_path}"

    # -----------------------------------------------------------
    # 3. Read SINGLE raw file
    # -----------------------------------------------------------
    raw_df = (
        spark.read.text(file_path)
        .withColumnRenamed("value", "RAW_RECORD")
    )

    # -----------------------------------------------------------
    # 4. Read metadata dynamically from DB
    # -----------------------------------------------------------
    metadata_df = (
        spark.table("FINCORE.TOTALDATAPARAMETERS")
        .filter(F.upper(F.col("FILE_TYPE")) == file_type)
        .filter(F.upper(F.col("TOBEINCLUDED")) == "Y")
        .select(
            F.upper(F.col("COLUMNNAME")).alias("COLUMNNAME"),
            F.col("STARTVALUE"),
            F.col("ENDVALUE"),
            F.upper(F.col("DATA_TYPE")).alias("DATA_TYPE")
        )
        .orderBy("STARTVALUE")
    )

    metadata = metadata_df.collect()

    if not metadata:
        raise Exception(f"No metadata found for FILE_TYPE = {file_type}")

    # -----------------------------------------------------------
    # 5. Build substring expressions dynamically
    # -----------------------------------------------------------
    select_exprs = []

    for row in metadata:
        col_name = row["COLUMNNAME"]
        start_pos = int(row["STARTVALUE"])
        end_pos = int(row["ENDVALUE"])
        data_type = row["DATA_TYPE"]

        length = end_pos - start_pos + 1
        base_col = F.substring(F.col("RAW_RECORD"), start_pos, length)

        if data_type in ("VARCHAR", "CHAR", "STRING"):
            final_col = base_col.cast(StringType())

        elif data_type in ("INT", "INTEGER"):
            final_col = base_col.cast(IntegerType())

        elif data_type in ("LONG", "BIGINT"):
            final_col = base_col.cast(LongType())

        elif data_type in ("DECIMAL", "NUMBER"):
            final_col = base_col.cast(DecimalType(38, 18))

        else:
            raise Exception(f"Unsupported DATA_TYPE: {data_type}")

        select_exprs.append(final_col.alias(col_name))

    # -----------------------------------------------------------
    # 6. Build FINAL dataframe (business columns only)
    # -----------------------------------------------------------
    final_df = raw_df.select(*select_exprs)

    # -----------------------------------------------------------
    # 7. Persist once â†’ avoid double computation
    # -----------------------------------------------------------
    final_df = final_df.persist()

    record_count = final_df.count()

    return final_df, target_delta_path, record_count


















from common.properties import get_hdfs_base


def get_delta_path_by_file_type(file_type):
    """
    Returns delta lake path based on FILE TYPE.
    Centralized here so ETL code stays clean.
    """

    HDFS_BASE = get_hdfs_base()
    file_type = file_type.upper()

    if file_type == "GLIF":
        return f"{HDFS_BASE}/DATA-LAKE/GLIF"

    elif file_type == "INV":
        return f"{HDFS_BASE}/DATA-LAKE/BANCS24/INV"

    elif file_type == "BOR":
        return f"{HDFS_BASE}/DATA-LAKE/BANCS24/BOR"

    elif file_type == "GLCCTA":
        return f"{HDFS_BASE}/DATA-LAKE/BANCS24/GLCCTA"

    elif file_type == "GLCCGEN":
        return f"{HDFS_BASE}/DATA-LAKE/BANCS24/GLCCGEN"

    else:
        raise Exception(f"Unsupported FILE TYPE: {file_type}")

