#!/usr/bin/env python3

from pyspark.sql import SparkSession, functions as F, types as T
from pyspark.sql.types import DecimalType
from delta.tables import DeltaTable
import traceback
import sys
from datetime import date, timedelta

def print_header(msg):
    print("\n" + "="*6 + " " + msg + " " + "="*6 + "\n")

def read_table_parallel_by_rownum(
    spark, jdbc_url, user, password, driver,
    difference, select_cols_sql, recon_date,
    num_partitions, fetchsize
):

    subquery = (
        "(\n"
        "  SELECT\n"
        "    {cols},\n"
        "    ROW_NUMBER() OVER (ORDER BY (BRANCH_CODE || '|' || CURRENCY || '|' || CGL)) rn\n"
        "  FROM {table}\n"
        "  WHERE RECON_RUN_DATE = TO_DATE('{date}','DD-MM-YY')\n"
        ") tmp"
    ).format(cols=select_cols_sql,
             table=difference,
             date=recon_date)

    print("################# PREDICATE PUSHDOWN QUERY #######################")
    print(subquery)
    print("##################################################################")

    ESTIMATED_UPPER_BOUND = 1000000

    df = (
        spark.read.format("jdbc")
            .option("url", jdbc_url)
            .option("dbtable", subquery)
            .option("user", user)
            .option("password", password)
            .option("driver", driver)
            .option("partitionColumn", "rn")
            .option("lowerBound", 1)
            .option("upperBound", ESTIMATED_UPPER_BOUND)
            .option("numPartitions", str(num_partitions))
            .option("fetchsize", str(fetchsize))
            .load()
    )
    return df

def main():
    try:
        spark = (
            SparkSession.builder
                .appName("TRUNCATE Delta Difference Table")
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .getOrCreate()
        )

        spark.conf.set("spark.sql.adaptive.enabled", "true")
        spark.conf.set("spark.sql.shuffle.partitions", "200")

        # --- Configuration ---
        today = date.today()
        yesterday = today - timedelta(days=1)

        ORACLE_QUERY_DATE = "20-11-25"
        TARGET_DATE_VALUE = yesterday
        HDFS_PATH = "hdfs://10.177.103.199:8022/data-lake/Fincore/DiffrenceTable"

        ORACLE_USER = "fincore"
        ORACLE_PASSWORD = "Password#1234"
        ORACLE_DRIVER = "oracle.jdbc.driver.OracleDriver"
        JDBC_URL = "jdbc:oracle:thin:@//10.177.103.192:1523/fincorepdb1"

        NUM_PARTITIONS_READ = 5
        FETCHSIZE = 20000

        print_header("TRUNCATE DELTA TABLE (REMOVE ALL DATA)")

        # -------- TRUNCATE DELTA TABLE --------
        if DeltaTable.isDeltaTable(spark, HDFS_PATH):
            delta_tbl = DeltaTable.forPath(spark, HDFS_PATH)
            delta_tbl.delete("true")
            print("All data removed successfully — schema retained.")
        else:
            print("Delta table does not exist at given HDFS path — no truncate required.")

        print_header("JOB COMPLETE")
        spark.stop()

    except Exception:
        print("ERROR during execution:")
        traceback.print_exc()
        try:
            spark.stop()
        except:
            pass
        sys.exit(1)

if __name__ == "__main__":
    print("START MAIN")
    main()
