from pyspark.sql.functions import col, substring, trim, lit
from pyspark.sql.types import StringType, DecimalType, DateType
from decimal import Decimal

# =====================================================
# Common imports (already present in your framework)
# =====================================================
spark = create_spark()                     # from common
HDFS_BASE = get_hdfs_base()                # from common
ORACLE_URL = oracle_url                   # from common
ORACLE_USER = oracle_user
ORACLE_PASSWORD = oracle_password
ORACLE_DRIVER = oracle_driver

# =====================================================
# Paths
# =====================================================
GLIF_INPUT_PATH = f"{HDFS_BASE}/glif/input/*"
GLIF_DELTA_PATH = f"{HDFS_BASE}/glif/delta"

# =====================================================
# Read GLIF file (.txt / .gz supported automatically)
# =====================================================
raw_df = spark.read.text(GLIF_INPUT_PATH) \
    .withColumnRenamed("value", "raw_record")

# =====================================================
# Read substring metadata from DB
# =====================================================
metadata_query = """
(
    SELECT
        COLUMNNAME,
        STARTVALUE,
        ENDVALUE,
        DATA_TYPE
    FROM FINCORE.TOTALDATAPARAMETERS
    WHERE TOBEINCLUDED = 'Y'
    ORDER BY STARTVALUE
) T
"""

metadata_df = spark.read.format("jdbc") \
    .option("url", ORACLE_URL) \
    .option("dbtable", metadata_query) \
    .option("user", ORACLE_USER) \
    .option("password", ORACLE_PASSWORD) \
    .option("driver", ORACLE_DRIVER) \
    .load()

metadata = metadata_df.collect()

if not metadata:
    raise Exception("GLIF metadata table is empty. Aborting.")

# =====================================================
# Build dynamic substring expressions
# =====================================================
select_exprs = []

for row in metadata:
    col_name = row["COLUMNNAME"]
    start_pos = int(row["STARTVALUE"])
    end_pos = int(row["ENDVALUE"])
    data_type = row["DATA_TYPE"].upper()

    length = end_pos - start_pos + 1

    expr_col = trim(
        substring(col("raw_record"), start_pos, length)
    )

    if data_type == "NUMBER":
        expr_col = expr_col.cast(DecimalType(25, 4))

    elif data_type == "DATE":
        expr_col = expr_col.cast(DateType())

    else:
        expr_col = expr_col.cast(StringType())

    select_exprs.append(expr_col.alias(col_name))

# =====================================================
# Create final parsed dataframe
# =====================================================
final_df = raw_df.select(*select_exprs)

# =====================================================
# Currency handling (INR default-safe)
# =====================================================
if "CURRENCY_CODE" in final_df.columns:
    final_df = final_df.withColumn(
        "CURRENCY_CODE",
        trim(col("CURRENCY_CODE"))
    )

# =====================================================
# Write to Delta (schema evolves via DB)
# =====================================================
final_df.write \
    .format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save(GLIF_DELTA_PATH)
