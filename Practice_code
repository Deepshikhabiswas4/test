import datetime 
from pyspark.sql import SparkSession , functions as F
import pyspark.sql.types as T
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType
from pyspark.sql.functions import to_date,expr, round, col,when, length, substring,trim, concat, lit, create_map, to_timestamp, sum,broadcast, coalesce,current_timestamp
from pyspark.sql.types import DecimalType, StringType, StructType, StructField, LongType, DateType, TimestampType
from datetime import date, timedelta
from pyspark.sql.functions import desc
import sys
from py4j.java_gateway import java_import
import concurrent.futures
import builtins
from pyspark.sql import Row
import re
# =========== Common Methods =============================
from common.logger import setup_logger 
from common.properties import get_oracle_properties
from common.dateUtil import get_etl_date
from common.fileUtil import files_to_read
from common.fileUtil import paths_for_read
from common.fileUtil import tables_to_read
from common.processRun import get_run_id
from common.pattern import read_patterns
from common.createSpark import create_spark_session
from common.fileUtil import get_hdfs_base
from common.check_monthend import check_MonthEnd
from datetime import datetime,date
from common.logdb import log_etl
from common.PPF_FCNB import ppf_postings
from common.PPF_FCNB import monthend_posting
from common.read_write_oracle import read_oracle
from common.read_write_oracle import write_oracle

HDFS_BASE = get_hdfs_base()
spark = create_spark_session("GLIF_PIPELINE", HDFS_BASE)
today = get_etl_date(spark)

yesterday = today - timedelta(days=1)
posting_date_str = today.strftime("%Y-%m-%d")
today_str = posting_date_str
yesterday=yesterday.strftime("%Y-%m-%d")
today= today.strftime("%Y-%m-%d")
RUN_ID = get_run_id( spark,posting_date_str,"GLIF") 
logger = setup_logger("GLIF", RUN_ID , posting_date_str)
today1 = date.today()   
logger.info(f"=== Using current date for POST_DATE: {posting_date_str} ===")
logger.info(f"RUN_ID as : {RUN_ID}")

logger.info("Fetched oracle properties SUCCESFULLY :)")
oracle_properties=get_oracle_properties()
logger.info(oracle_properties)

oracle_url = oracle_properties["url"]
oracle_user = oracle_properties["user"]
oracle_password = oracle_properties["password"]
oracle_driver = oracle_properties["driver"]
DriverManager = spark._jvm.java.sql.DriverManager
startTime= datetime.now()
 

log_etl(spark,"GLIF",RUN_ID , "GLIF ETL PRCOESS STARTED",5,1,startTime)

fncb_cgl_query="(SELECT FROM_CGL,TO_CGL,CGL_TYPE FROM FCNB_CGL) T1" 

fcnb_cgl_df = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", fncb_cgl_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .load()

fcnb_currency_query="(SELECT CURRENCY_CODE,DEPOSIT_EXPENSE_FLAG,LOAN_INCOME_FLAG FROM FCNB_CURRENCY_CONFIG) T1"

fcnb_currency = spark.read.format("jdbc") \
    .option("url", oracle_url) \
    .option("dbtable", fcnb_currency_query) \
    .option("user", oracle_user) \
    .option("password", oracle_password) \
    .option("driver", oracle_driver) \
    .load()



batch_id_schema = StructType([
    StructField("BATCH_ID_VAL", LongType(), False) 
])

BATCH_ID_LITERAL = None
try:
    df_batch_id = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("query", "SELECT get_next_batch_id AS BATCH_ID_VAL FROM DUAL") \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .option("fetchsize",1).load()

    batch_id_value = df_batch_id.collect()[0]["BATCH_ID_VAL"]
    BATCH_ID_LITERAL = int(batch_id_value)
    logger.info(f"=== Fetched BATCH_ID: {BATCH_ID_LITERAL} ===")
except Exception as e:
    logger.info(f"Error fetching BATCH_SEQ via JDBC: {e}")
    log_etl(spark,"GLIF",RUN_ID , "ERROR IN reading frm db : 201 : 361",10,3,startTime)
    spark.stop()
    exit()

sc = spark.sparkContext
# 1. Setup Hadoop FileSystem Access
jvm = sc._jvm
jsc = sc._jsc
fs = jvm.org.apache.hadoop.fs.FileSystem.get(jsc.hadoopConfiguration())
Path = jvm.org.apache.hadoop.fs.Path


hdfs_paths = paths_for_read("glif",today,today_str)

HDFS_BASE = hdfs_paths["HDFS_BASE"]
HDFS_PATH = hdfs_paths["HDFS_PATH"]
GL_DATALAKE_PATH = hdfs_paths["GL_DATALAKE_PATH"]
manifest_filename = hdfs_paths["manifest_filename"]
spark_stream_path = hdfs_paths["spark_stream_path"] + manifest_filename

oracle_tables = tables_to_read("glif")


try:
    log_etl(spark,"GLIF",RUN_ID , "File Reading Started",8,1,startTime)
    files = files_to_read(spark,"glif")
    logger.info(files)
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID , "ERROR READING FILES : 202 : 388",8,3,startTime)
    spark.stop()
    exit()

file_types_with_extra = files["stream_files"]
file_types_without_extra = files["non_stream_files"]

paths_to_read = [f"{HDFS_PATH}/{ft}_31122025_*_[0-9][0-9][0-9].gz" for ft in file_types_with_extra]
paths_to_read.extend([f"{HDFS_PATH}/{ft}_31122025_*[!0-9].gz" for ft in file_types_without_extra])


TRANSACTIONS = oracle_tables["TRANSACTIONS"]
BALANCE = oracle_tables["BALANCE"]
INVALID = oracle_tables["INVALID"]
CGLS = oracle_tables["CGLS"]
CURRENCY = oracle_tables["CURRENCY"]
BRANCH = oracle_tables["BRANCH"]

logger.info(f"ETL date : {today}")
logger.info(f"Oracle properties fetched successfully")

try:
    df_raw = read_patterns(spark,paths_to_read,posting_date_str,RUN_ID)
    log_etl(spark,"GLIF",RUN_ID , "File Reading Completed",9,2,startTime)
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID , "ERROR READING FILES : 203 : 411",8,3,startTime)
    spark.stop()
    exit()


logger.info(f"Reading Patterns")
log_etl(spark,"GLIF",RUN_ID , "Analytics Started",10,1,startTime)
df_processed = df_raw.withColumns({
    "Id": expr("substring(value, 51, 18)"),
    "currency_code": expr("substring(value, 48, 3)")
})
df_clean = df_processed.withColumns({
    "Amount_raw": when(col("currency_code") != "INR", expr("substring(value, 133, 17)"))
                 .otherwise(expr("substring(value, 116, 17)")),
    "last_char": when(col("currency_code") != "INR", expr("substring(value, 149, 1)"))
                 .otherwise(expr("substring(value, 132, 1)"))
}).drop("value", "currency_code")

df_clean = df_clean.withColumns({
    "Amount_base": substring(col("Amount_raw"), 1, 16),
    "sign_char": substring(col("Amount_raw"), 17, 1)
})

digit_map = create_map(
    lit('1'), lit('1'), lit('2'), lit('2'), lit('3'), lit('3'), lit('4'), lit('4'), lit('5'), lit('5'),
    lit('6'), lit('6'), lit('7'), lit('7'), lit('8'), lit('8'), lit('9'), lit('9'), lit('0'), lit('0'),
    lit('p'), lit('0'), lit('q'), lit('1'), lit('r'), lit('2'), lit('s'), lit('3'), lit('t'), lit('4'),
    lit('u'), lit('5'), lit('v'), lit('6'), lit('w'), lit('7'), lit('x'), lit('8'), lit('y'), lit('9')
)

df_with_signed_amount = df_clean.withColumns({
    "last_digit": digit_map.getItem(col("sign_char")),
    "sign": when(col("sign_char").isin(['p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y']), -1).otherwise(1)
})
df_with_signed_amount = df_with_signed_amount.withColumn(
    "Amount_decimal_str",
    concat(col("Amount_base"), col("last_digit"))
)
df_final = df_with_signed_amount.withColumns({
    "Amount_final": (col("Amount_decimal_str").cast(DecimalType(25,4)) / 1000) * col("sign")
})
df_with_signed_amount = df_final.withColumns({
    "Amountpve": when(df_final["Amount_final"] > 0, df_final["Amount_final"]).otherwise(0),
    "Amountnve": when(df_final["Amount_final"] < 0, df_final["Amount_final"]).otherwise(0)
})

try:
    df_agg = df_with_signed_amount.groupBy("Id").agg(
        F.sum(F.col("Amountpve")).alias("DEBIT_AMOUNT"),
        F.sum(F.col("Amountnve")).alias("CREDIT_AMOUNT"),
        F.count(F.col("Id")).alias("TRANSACTION_COUNT")
    )
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID , "ERROR in grpby  : 204 : 464",10,3,startTime)
    spark.stop()
    exit() 

df_mapped = df_agg.withColumns({ 
    "BATCH_ID": lit(BATCH_ID_LITERAL).cast(StringType()),
    "JOURNAL_ID": lit(None).cast(StringType()),
    "BRANCH_CODE": substring(col("Id"), 1, 5),
    "CURRENCY": substring(col("Id"), 6, 3),
    "CGL": substring(col("Id"), 9, 10),
    "NARRATION": lit("CBS consolidated txns").cast(StringType()),
    "SOURCE_FLAG": lit("C").cast(StringType())
})

df_final_schema = df_mapped.select(
    "BATCH_ID",
    "JOURNAL_ID",
    "BRANCH_CODE",
    "CURRENCY",
    "CGL",
    "NARRATION",
    "DEBIT_AMOUNT",
    "CREDIT_AMOUNT",
    "TRANSACTION_COUNT",
    "SOURCE_FLAG"
)
logger.info(f"Raw Data Got Filtered")
df_final_schema.cache()

# ===================================================================================
                #    || BRANCH CODE ||
# ===================================================================================

try:
    branch_query = f"""
    (
        SELECT code FROM {BRANCH} 
    ) T1
    """
    branch_list = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", branch_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .load()
    logger.info(f"Branch list loaded successfully")
except Exception as e:
    logger.error("Error {e} while fetching the branchlist from oracle.")
    log_etl(spark,"GLIF",RUN_ID , "ERROR in datafetching  : 205 : 513",10,3,startTime)
    spark.stop()
    exit()
try:    
    df_validated_branch = df_final_schema.join(
        broadcast(branch_list.select(col("CODE").alias("BRANCH_CODE")).withColumn("IS_VALID_BRANCH", lit(1))),
        on="BRANCH_CODE",
        how="left"
    )
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID , "ERROR in join  : 206 : 523",10,3,startTime)
    spark.stop()
    exit()

df_invalid_branches = df_validated_branch.filter(col("IS_VALID_BRANCH").isNull()).withColumn("REASON",lit("INVALID BRANCH")).select(
"BRANCH_CODE",
"BATCH_ID",
"CURRENCY",
"CGL",
"NARRATION",
"DEBIT_AMOUNT",
"CREDIT_AMOUNT",
"TRANSACTION_COUNT",
"SOURCE_FLAG",
"REASON"    
)

df_valid_branches = df_validated_branch.filter(col("IS_VALID_BRANCH").isNotNull())

# # ===================================================================================
#                 #    || CURRENCY ||
# # ===================================================================================
try:
    currency_query = f"""
    (
        SELECT CURRENCY_CODE,CURRENCY_RATE from {CURRENCY}
    ) T1
    """
    currency_list = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", currency_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .load()
    logger.info("Sucessfully currencylist loaded.")
except Exception as e:
    logger.error("Error {e} while fetching the currencylist from oracle.")
    log_etl(spark,"GLIF",RUN_ID , "ERROR in join  : 207: 561",10,3,startTime)
    spark.stop()
    exit()


df_validated_currency = df_valid_branches.join(
    broadcast(currency_list.select(col("CURRENCY_CODE").alias("CURRENCY")).withColumn("IS_VALID_CURRENCY", lit(1))),
    on="CURRENCY",
    how="left"
).withColumn(
    "VALIDATED_CURRENCY",
    when(col("IS_VALID_CURRENCY").isNotNull(), col("CURRENCY"))
    .otherwise(lit("invalid_CURRENCY"))
)


df_invalid_currency = df_validated_currency.where(col("VALIDATED_CURRENCY") == "invalid_CURRENCY").withColumn("REASON",lit("INVALID CURRENCY")).select(
"BRANCH_CODE",
"BATCH_ID",
"CURRENCY",
"CGL",
"NARRATION",
"DEBIT_AMOUNT",
"CREDIT_AMOUNT",
"TRANSACTION_COUNT",
"SOURCE_FLAG",
"REASON"    
)

df_valid_currency = df_validated_currency.where(col("VALIDATED_CURRENCY") != "invalid_CURRENCY")

df_valid_currency =  df_valid_currency.drop(col("VALIDATED_CURRENCY"),col("IS_VALID_CURRENCY"))


invalid_df = df_invalid_branches.unionByName(df_invalid_currency).withColumn("DATE",F.to_date(F.lit(posting_date_str)))

# # ===================================================================================
#                 #    || CGL ||
# # ===================================================================================
try:
    cgl_query = f"""
    (
        SELECT CGL_NUMBER FROM {CGLS} 
    ) T1
    """
    master_cgl_list = spark.read.format("jdbc") \
        .option("url", oracle_url) \
        .option("dbtable", cgl_query) \
        .option("user", oracle_user) \
        .option("password", oracle_password) \
        .option("driver", oracle_driver) \
        .load()
    logger.info("Sucessfully cgl list loaded")
except Exception as e:
    logger.error("Error {e} while loading cgl list")  
    log_etl(spark,"GLIF",RUN_ID , "ERROR in fetching data   : 208: 617",10,3,startTime)
    spark.stop()
    exit()

try: 
    df_validated = df_valid_currency.join(
        broadcast(master_cgl_list.select(col("CGL_NUMBER").alias("CGL")).withColumn("IS_VALID", lit(1))),
        on="CGL",
        how="left"
    ).withColumn(
        "VALIDATED_CGL",
        when(col("IS_VALID").isNotNull(), col("CGL"))
        .when(col("CGL").startswith("5"), lit("5000000000"))
        .otherwise(lit("1111111111"))
    ).withColumn(
        "NARRATION1",   
        when(col("IS_VALID").isNull(), 
             concat(lit("INVALID CGL: "), col("CGL"), lit(" - "), col("NARRATION")))
        .otherwise(col("NARRATION"))
    )
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID , "ERROR in join  : 208: 638",10,3,startTime)
    spark.stop()
    exit()



import pyspark.sql.functions as F


try:
    result = df_validated.groupBy("VALIDATED_CGL", "CURRENCY", "BRANCH_CODE").agg(
        F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
        F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
        F.sum("TRANSACTION_COUNT").alias("TRANSACTION_COUNT"),
        F.first("NARRATION1").alias("NARRATION"),          
        F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),      
        F.first("BATCH_ID").alias("BATCH_ID"),
        F.first("JOURNAL_ID").alias("JOURNAL_ID"),
    ).select(F.col("VALIDATED_CGL").alias("CGL"), "CURRENCY", "BRANCH_CODE","NARRATION", "SOURCE_FLAG", "BATCH_ID", "DEBIT_AMOUNT", "CREDIT_AMOUNT", "TRANSACTION_COUNT","JOURNAL_ID")
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID , "ERROR in grpby  : 209: 658",10,3,startTime)
    spark.stop()
    exit()

# ===================================================================================
#    || Balancing and PPF Logic ||
# ===================================================================================

result_net1 = result.withColumn(
    "check",
    when(substring(col("CGL"), 1, 1) == "5", lit("5000000000"))
    .otherwise(lit("1111111111"))
)

try:
    new_result =result_net1.groupBy("BRANCH_CODE","CURRENCY",col("check").alias("CGL")).agg(
        F.sum("DEBIT_AMOUNT").alias("DEBIT_AMOUNT"),
        F.sum("CREDIT_AMOUNT").alias("CREDIT_AMOUNT"),
        F.first("NARRATION").alias("NARRATION"),          
        F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),      
        F.first("BATCH_ID").alias("BATCH_ID"),
        F.first("JOURNAL_ID").alias("JOURNAL_ID"),
        F.first("TRANSACTION_COUNT").alias("TRANSACTION_COUNT")
        ).withColumn(
        "NET", 
        col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")
    )
except Exception as e:
    log_etl(spark,"GLIF", RUN_ID , "ERROR in grpby  : 210: 686",10,3,startTime)
    spark.stop()
    exit()


synthetic = new_result.filter(col("NET") != 0).select(
    col("CGL"),col("CURRENCY"),col("BRANCH_CODE"),
    when(col("NET") < 0, -col("NET")).otherwise(lit(0)).alias("DEBIT_AMOUNT"),
    when(col("NET") > 0, -col("NET")).otherwise(lit(0)).alias("CREDIT_AMOUNT"),
    col("TRANSACTION_COUNT"),
    F.lit("OUT OF BALANCE").alias("NARRATION"),
    col("SOURCE_FLAG"),
    col("BATCH_ID"),
    col("JOURNAL_ID"))

try:
    final_balanced = result.unionByName(synthetic).withColumn("TRANSACTION_DATE",F.to_date(F.lit(posting_date_str)))
    final_balanced = result.unionByName(synthetic)
except Exception as e:
    log_etl(spark,"GLIF",RUN_ID ,"ERROR in uniopn : 211: 705",10,3,startTime)
    spark.stop()
    exit()

PPF_Posting_df = ppf_postings(final_balanced)
PPF_Posting_df.cache()


PPF_Posting = PPF_Posting_df.withColumn("TRANSACTION_DATE",F.to_date(F.lit(posting_date_str)))

log_etl(spark,"GLIF",RUN_ID , "Transactions Calculated",11,2,startTime)
# ===============================================================================
# Journal entries data from the database
# ===============================================================================

filter_date_str = yesterday


if(RUN_ID>1):
    sql_query = f"""(
                        SELECT BRANCH_CODE, CURRENCY, CGL,DEBIT_AMOUNT,CREDIT_AMOUNT,BATCH_ID,JOURNAL_ID,NARRATION,TRANSACTION_COUNT,SOURCE_FLAG 
                        FROM {TRANSACTIONS} 
                        WHERE TRUNC(TRANSACTION_DATE) = TO_DATE('{filter_date_str}', 'YYYY-MM-DD') and SOURCE_FLAG='J'
                    ) T1"""
    try:
        journal_entries = read_oracle(spark,sql_query)
        # spark.read.format("jdbc") \
        #     .option("url", oracle_url) \
        #     .option("dbtable", sql_query) \
        #     .option("user", oracle_user) \
        #     .option("password", oracle_password) \
        #     .option("driver", oracle_driver) \
        #     .load()
        logger.info("Journal entries loaded from db for {filter_date_str} date.")
    except Exception as e:
        logger.error("Error {e} while loading Journal entries for {filter_date_str} date.")
        log_etl(spark,"GLIF",RUN_ID , "ERROR in db reading  : 212: 741",10,3,startTime)
        spark.stop()
        exit()

    PPF_Posting_df = PPF_Posting_df.unionByName(journal_entries).groupBy("CGL","BRANCH_CODE","CURRENCY").agg(
        F.sum(col("DEBIT_AMOUNT")).alias("DEBIT_AMOUNT"),
        F.sum(col("CREDIT_AMOUNT")).alias("CREDIT_AMOUNT"),
        F.sum(col("TRANSACTION_COUNT")).alias("TRANSACTION_COUNT"),
        F.first("NARRATION").alias("NARRATION"),         
        F.first("SOURCE_FLAG").alias("SOURCE_FLAG"),      
        F.first("BATCH_ID").alias("BATCH_ID"),
        F.first("JOURNAL_ID").alias("JOURNAL_ID"),
        ).select("BRANCH_CODE", "CURRENCY", "CGL","DEBIT_AMOUNT","CREDIT_AMOUNT","BATCH_ID","JOURNAL_ID","NARRATION","TRANSACTION_COUNT","SOURCE_FLAG")

processed_df = PPF_Posting_df.withColumn(
    "BALANCE", 
    col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")
).select("CGL", "BALANCE","CURRENCY","BRANCH_CODE")

# ====================================================================================================================================================================================================================================================================

# ====================================================================================================================================================================================================================================================================


processed_df=processed_df.select(concat(
    col("BRANCH_CODE"),
    col("CURRENCY"),
    col("CGL")).alias("glcc"), 
    col("BALANCE"))

try:
    df_gl = spark.read.format("delta").load(GL_DATALAKE_PATH).filter(col("BALANCE_DATE") == yesterday)
    df_gl = df_gl.select(col("GLCC").alias("glcc"),col("closing_balance").cast(DecimalType(25,4)).alias("BALANCE"))
    logger.info("Data loaded from deltalake.")
except Exception as e:
        try:
            yesterday=yesterday.strftime("%Y-%m-%d")
            BALANCE_query = f"""(select CGL, CURRENCY, BRANCH_CODE, BALANCE from GL_BALANCE_ST where BALANCE_DATE = TO_DATE('{yesterday}', 'YYYY-MM-DD')) a"""
            gl_yesterday = read_oracle(spark,BALANCE_query)
            # spark.read \
            #     .format("jdbc") \
            #     .option("url", oracle_url) \
            #     .option("dbtable", BALANCE_query) \
            #     .option("user", oracle_user) \
            #     .option("password", oracle_password) \
            #     .option("driver", oracle_driver) \
            #     .load()     
            print("yesterday gl")
            print("yesterday gl")
            df_gl= gl_yesterday.select(concat(col("BRANCH_CODE"),col("CURRENCY"),col("CGL")).alias("glcc"), col("BALANCE"))
            
        except Exception as e:
        
            df_gl=spark.createDataFrame([],schema="""glcc STRING, BALANCE DECIMAL(25,4)""")
            logger.error("Error {e} while loading data from delta lake.")
            log_etl(spark,"GLIF",RUN_ID , "Error while loading data from delta lake. : 213: 792",10,3,startTime)
            spark.stop()
try:
    combined_df = processed_df.unionAll(df_gl)
except Exception as e:
    logger.error("Error in union")
    log_etl(spark,"GLIF",RUN_ID , "Error while union : 214: 800",10,3,startTime)
    spark.stop()
    exit()
final_aggregated_df = combined_df.groupBy("glcc").agg(sum("BALANCE").alias("BALANCE"))

final_aggregated_df=final_aggregated_df.withColumn("BALANCE_DATE",F.to_date(F.lit(posting_date_str)))

dl_df=final_aggregated_df.select("glcc",col("BALANCE").alias("closing_balance").cast(DecimalType(25,4)),"BALANCE_DATE")


# ======================================================================================================================================================
#                                         Month End FCNB 
# ======================================================================================================================================================

   

monthend = check_MonthEnd()
logger.info(f"monthend is {monthend}")
if(monthend):
    FCNB_transactions = monthend_posting(dl_df,fcnb_cgl_df,fcnb_currency,currency_list,posting_date_str)
    FCNB_result=FCNB_transactions.withColumn(
    "BALANCE", 
    col("DEBIT_AMOUNT") + col("CREDIT_AMOUNT")
    ).select(concat(col("BRANCH_CODE"),col("CURRENCY"),col("CGL")).alias("glcc"),col("BALANCE").alias("closing_balance"), F.lit(posting_date_str).cast(DateType()).alias("BALANCE_DATE"))
    dl_df = dl_df.unionAll(FCNB_result)
    

# # # ==============================================================================================================
dl_df =dl_df.groupBy("glcc").agg(
    sum("closing_balance").alias("BALANCE"),
    F.first("BALANCE_DATE").alias("BALANCE_DATE")
).select("glcc","BALANCE","BALANCE_DATE")


df_gl_balance_final = dl_df.withColumns({ 
    "BRANCH_CODE": substring(F.col("glcc"), 1, 5),
    "CURRENCY": substring(F.col("glcc"), 6, 3),
    "CGL": substring(F.col("glcc"), 9, 10),
}).select("BRANCH_CODE","CURRENCY","CGL","BALANCE_DATE","BALANCE")

df_converted = (
   df_gl_balance_final
   .join(
       broadcast(currency_list),
       df_gl_balance_final["CURRENCY"] == currency_list["CURRENCY_CODE"],
       "left"
   )
   .select(
       col("BRANCH_CODE"),
       col("CURRENCY"),
       col("CGL"),
       col("BALANCE_DATE"),
       col("BALANCE"),
       col("CURRENCY_RATE")  
   )
   .withColumn(
       "APPLIED_RATE",
       when(col("CURRENCY") == "INR", lit(1))
       .otherwise(coalesce(col("CURRENCY_RATE"), lit(1)))
   )
   .withColumn(
       "INR_BALANCE",
       round(col("BALANCE") * col("APPLIED_RATE"),2).cast(DecimalType(25, 2))
   )
)
balance_df = df_converted.select(
   "CGL",
   "CURRENCY",
   "BRANCH_CODE",
   "BALANCE",
   "INR_BALANCE",
   "BALANCE_DATE"
)
balance_df.cache()

opening_balance_df = balance_df.select("CGL",
   "CURRENCY",
   "BRANCH_CODE",
   "BALANCE",
   "INR_BALANCE").F.lit(tommorrow_date).cast(DateType()).alias("BALANCE_DATE")



# ==================================================================================

deltalake_final = balance_df.select(concat(col("BRANCH_CODE"),col("CURRENCY"),col("CGL")).alias("glcc"),col("BALANCE").cast(DecimalType(25,4)).alias("closing_balance"), F.lit(posting_date_str).cast(DateType()).alias("BALANCE_DATE"))
# ==================================================================================
log_etl(spark,"GLIF",RUN_ID , "Balance Calculated",12,2,startTime) 
try:
    deltalake_final.write.format("delta").mode("append").save(GL_DATALAKE_PATH)
except Exception as e:
    logger.info(f"Error during saving in datalake  {e}")
    log_etl(spark,"GLIF",RUN_ID ,"Error saving gl_balance : 216: 910",14,3,startTime)
    spark.stop()
    exit()
