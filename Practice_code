from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, lit, concat, broadcast
)
from pyspark.sql.types import DateType, DecimalType
from datetime import timedelta
import pyspark.sql.functions as F

# Common Utility Imports (Your Existing Framework)
from common.read_write_oracle import read_oracle, write_oracle
from common.properties import get_hdfs_base


# ============================================
# Spark Session
# ============================================

spark = SparkSession.builder.appName("GLIF_OpeningBalance_ETLplus1").getOrCreate()

logger.info("=== Opening Balance Forward Job Started ===")


# ============================================
# Paths & Table Names
# ============================================

HDFS_BASE = get_hdfs_base()

GL_BALANCE_TABLE = "GL_BALANCE"
BALANCE_TABLE = "BALANCE"

GL_DATALAKE_PATH = f"{HDFS_BASE}/glif/opening_balance/"


# ============================================
# ETL Date Logic
# ============================================

etl_date = get_etl_date(spark)
etl_plus_1_date = etl_date + timedelta(days=1)

etl_date_str = etl_date.strftime("%Y-%m-%d")
etl_plus_1_str = etl_plus_1_date.strftime("%Y-%m-%d")

logger.info("============================================")
logger.info(f"ETL Closing Balance Date   : {etl_date_str}")
logger.info(f"ETL+1 Opening Balance Date : {etl_plus_1_str}")
logger.info("============================================")


# ============================================
# Step 1: Read Closing Balance from GL_BALANCE
# ============================================

logger.info("=== Reading Closing Balance from Oracle GL_BALANCE ===")

gl_balance_query = f"""
(
    SELECT
        CGL,
        CURRENCY,
        BRANCH_CODE,
        BALANCE,
        INR_BALANCE
    FROM {GL_BALANCE_TABLE}
    WHERE BALANCE_DATE = TO_DATE('{etl_date_str}', 'YYYY-MM-DD')
) T1
"""

closing_balance_df = read_oracle(spark, gl_balance_query)

logger.info("=== Closing Balance Loaded Successfully ===")


# ============================================
# Step 2: Create Opening Balance for ETL+1
# ============================================

opening_balance_df = closing_balance_df.withColumn(
    "BALANCE_DATE",
    lit(etl_plus_1_str).cast(DateType())
)

logger.info("=== Opening Balance DF Created for ETL+1 ===")


# ============================================
# Step 3: Year-End Exception Rule (31st March)
# ============================================

if etl_date.month == 3 and etl_date.day == 31:

    logger.info("=== Year End Detected: Applying BAL_FWD Rule ===")

    # Step 3.1: Active Financial Year Check
    cal_query = """
    (SELECT ACTIVE_FLAG FROM CALENDER_CONFIG WHERE ACTIVE_FLAG = 1) T1
    """

    cal_active = read_oracle(spark, cal_query)

    if cal_active.count() > 0:

        logger.info("=== Active Financial Year Found ===")

        # Step 3.2: Load BAL_FWD flag from CGL_MASTER
        cgl_query = """
        (SELECT CGL_NUMBER, BAL_FWD FROM CGL_MASTER) T1
        """

        cgl_master = read_oracle(spark, cgl_query)

        # Step 3.3: Apply BAL_FWD Filter
        opening_balance_df = opening_balance_df.join(
            broadcast(cgl_master),
            opening_balance_df["CGL"] == cgl_master["CGL_NUMBER"],
            "inner"
        ).filter(
            col("BAL_FWD") == 1
        ).select(opening_balance_df["*"])

        logger.info("=== BAL_FWD Filter Applied Successfully ===")

    else:
        logger.info("=== No Active Financial Year Found. Skipping BAL_FWD Filter ===")


# ============================================
# Step 4: Write Opening Balance into Delta Lake
# ============================================

logger.info("=== Writing Opening Balance into Delta Lake ===")

deltalake_final = opening_balance_df.select(
    concat(col("BRANCH_CODE"), col("CURRENCY"), col("CGL")).alias("GLCC"),
    col("BALANCE").cast(DecimalType(25, 4)).alias("CLOSING_BALANCE"),
    col("BALANCE_DATE")
)

try:
    deltalake_final.write.format("delta") \
        .mode("append") \
        .save(GL_DATALAKE_PATH)

    logger.info("=== Opening Balance Saved into Delta Lake Successfully ===")

except Exception as e:
    logger.error(f"Delta Write Failed: {e}")
    spark.stop()
    exit()


# ============================================
# Step 5: Write Opening Balance into Oracle BALANCE Table
# ============================================

logger.info("=== Writing Opening Balance into Oracle BALANCE Table ===")

db_status = write_oracle(opening_balance_df, BALANCE_TABLE)

logger.info(db_status)


# ============================================
# Job End
# ============================================

logger.info("=== Opening Balance Forward Job Completed Successfully ===")
spark.stop()
